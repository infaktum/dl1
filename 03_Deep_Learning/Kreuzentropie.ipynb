{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df2b9c0a-8818-4dc9-b734-fad0f70a0170",
   "metadata": {},
   "source": [
    "# Kreuzentropie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f37f98d-dcc1-463d-bc44-69f3bf7ff463",
   "metadata": {},
   "source": [
    "Die Kreuzentropie (_Cross-Entropy_) ist eine gängige Verlustfunktion im überwachten Lernen, insbesondere für Klassifikationsprobleme. Sie misst, wie unterschiedlich zwei Wahrscheinlichkeitsverteilungen sind – typischerweise die wahre Verteilung der Klassenlabels und die vorhergesagte Verteilung eines Modells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8616bc72-cb1d-4003-8feb-b7810f7ac241",
   "metadata": {},
   "source": [
    "## Mathematische Definition\n",
    "Für ein einzelnes Beispiel mit einer Klasse $y$ (one-hot-encoded)  und einer vorhergesagten Wahrscheinlichkeitsverteilung $\\hat{y}$\n",
    "wird die Kreuzentropie definiert als:\n",
    "\n",
    "$$ H(y,\\hat{y}) = - \\sum y_i log(\\hat{y_i}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82b607a-4c0e-481e-8e60-332887af6ab6",
   "metadata": {},
   "source": [
    "Da $y$ eine One-Hot-Kodierung ist (nur eine Klasse $k$ hat den Wert 1, alle anderen 0), vereinfacht sich die Formel zu:\n",
    "\n",
    "$$ H(y,\\hat{y}) = -log(\\hat{y}_k) $$\n",
    "\n",
    "Für eine gesamte Trainingsmenge mit $m$ Beispielen sieht die mittlere Kreuzentropie so aus:\n",
    "$$ L = -\\frac{1}{m} \\sum_j \\sum_i y_{ji}log(\\hat{y}_{ji})   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4943856a-2eed-4483-bec4-ff3efdfcc043",
   "metadata": {},
   "source": [
    "### Binary Cross Entropy (BCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2488f9c-4e73-4cdf-80c5-727f5766d3af",
   "metadata": {},
   "source": [
    "Für ein binäres Problem mit einem einzelnen Label $y \\epsilon \\{0,1\\}$ und einer vorhergesagten Wahrscheinlichkeit $\\hat{y}$ vereinfacht sich die Kreuzentropie zu:\n",
    "$$   H(y,\\hat{y}) = - [y \\log(\\hat{y}) \\; + \\;(1-y) \\log(1 - \\hat{y})] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a606199d-f65e-4412-9bce-cd83c8ff8b37",
   "metadata": {},
   "source": [
    "### Warum wird die Kreuzentropie verwendet?\n",
    "* Strafung falscher Vorhersagen: Wenn das Modell einer falschen Klasse eine hohe Wahrscheinlichkeit zuweist, wird der Fehler stark bestraft (da $\\log(x)$  für kleine $x$ stark negativ wird).\n",
    "* Anpassung an Wahrscheinlichkeitsverteilungen: Sie ist besonders gut geeignet für Softmax-Ausgaben, da sie Wahrscheinlichkeiten direkt bewertet.\n",
    "* Vermeidung von Ungenauigkeiten bei Gradientenabstiegen: Die Kreuzentropie hat bessere numerische Eigenschaften als z. B. der quadratische Fehler für Klassifikationsprobleme."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52e2c0d-4cde-4dbd-aeb6-af7017a1bbee",
   "metadata": {},
   "source": [
    "Die Kreuzentropie ist eine effektive Verlustfunktion für Klassifikationsprobleme, da sie die Wahrscheinlichkeit für korrekte Vorhersagen maximiert und das Lernen effizienter macht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65cb9e1-4f89-493a-87e7-2a3bb1c7a893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7263651a-5bc4-4411-b041-6b20d30d1d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
